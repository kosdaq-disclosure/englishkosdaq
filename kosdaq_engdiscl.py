import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
import time

# í˜ì´ì§€ ì„¤ì • (wide ëª¨ë“œë¡œ ì„¤ì •)
st.set_page_config(
    page_title="ì˜¤ëŠ˜ì˜ ì½”ìŠ¤ë‹¥ ë²ˆì—­ëŒ€ìƒ ê³µì‹œ",
    layout="wide",  # í™”ë©´ì„ ë„“ê²Œ ì‚¬ìš©
    initial_sidebar_state="collapsed"  # ì‚¬ì´ë“œë°” ì´ˆê¸° ìƒíƒœë¥¼ ì ‘íŒ ìƒíƒœë¡œ ì„¤ì •
)

# ì œëª© ì„¤ì •
st.title('ì˜¤ëŠ˜ì˜ ì½”ìŠ¤ë‹¥ ë²ˆì—­ëŒ€ìƒ ê³µì‹œ')

#-----------------------------------------------------------
# CSV íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
#-----------------------------------------------------------

# ê³µì‹œì„œì‹ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
@st.cache_data
def load_kosdaq_format_data():
    try:
        df = pd.read_csv("kosdaq_format.csv", dtype=str)
        return df
    except Exception as e:
        st.error(f"ê³µì‹œì„œì‹ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

# íšŒì‚¬ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
@st.cache_data
def load_kosdaq_company_data():
    try:
        df = pd.read_csv("kosdaq_company.csv", dtype=str)
        return df
    except Exception as e:
        st.error(f"íšŒì‚¬ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

# ë°ì´í„° ë¡œë“œ
df_svc = load_kosdaq_format_data()
df_listed = load_kosdaq_company_data()
df_listed['íšŒì‚¬ì½”ë“œ'] = df_listed['íšŒì‚¬ì½”ë“œ'].astype(str).str.zfill(5)

# 2ê°œì˜ ì¹¼ëŸ¼ ìƒì„± (ì—…ë¡œë“œ ì„¹ì…˜ ì œê±°)
col1, col2 = st.columns(2)

# ì²«ë²ˆì§¸ ì¹¼ëŸ¼: ì§€ì›ëŒ€ìƒê³µì‹œì„œì‹ê¸°ì¤€
with col1:
    st.subheader('ì§€ì›ëŒ€ìƒ ê³µì‹œì„œì‹')
    if not df_svc.empty:
        st.write(f'{len(df_svc)}ê°œ')
        st.dataframe(df_svc, use_container_width=True)
    else:
        st.warning("ê³µì‹œì„œì‹ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ë‘ë²ˆì§¸ ì¹¼ëŸ¼: ì§€ì›ëŒ€ìƒ íšŒì‚¬ëª©ë¡
with col2:
    st.subheader('ì§€ì›ëŒ€ìƒ íšŒì‚¬ ëª©ë¡')
    if not df_listed.empty:
        st.write(f'{len(df_listed)}ì‚¬')
        st.dataframe(df_listed, use_container_width=True)
    else:
        st.warning("íšŒì‚¬ ëª©ë¡ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ë‚ ì§œ ê³„ì‚° í•¨ìˆ˜
def get_default_date():
    today = datetime.today()
    if today.weekday() in [5, 6]:  # í† ìš”ì¼(5) ë˜ëŠ” ì¼ìš”ì¼(6)
        return (today - timedelta(days=today.weekday() - 4)).strftime("%Y-%m-%d")  # ì§ì „ ê¸ˆìš”ì¼
    return today.strftime("%Y-%m-%d")  # ì˜¤ëŠ˜ ë‚ ì§œ

# ì œëª©
st.subheader('ì¡°íšŒì¼ì ì„ íƒ')

# ë‚ ì§œ ì„ íƒ ìœ„ì ¯ ì¶”ê°€
selected_date = st.date_input(
    "ì¡°íšŒí•  ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”",
    value=datetime.strptime(get_default_date(), "%Y-%m-%d"),
    min_value=datetime(2020, 1, 1),  # ìµœì†Œ ì„ íƒ ê°€ëŠ¥ ë‚ ì§œ
    max_value=datetime.today()       # ìµœëŒ€ ì„ íƒ ê°€ëŠ¥ ë‚ ì§œ
)

# ì„ íƒëœ ë‚ ì§œë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
today_date = selected_date.strftime("%Y-%m-%d")
        
# ë²„íŠ¼ ìƒì„±
if st.button('ì½”ìŠ¤ë‹¥ ì˜ë¬¸ê³µì‹œ ì§€ì›ëŒ€ìƒ ê³µì‹œì¡°íšŒ'):
    # ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    if df_svc.empty or df_listed.empty:
        st.error("í•„ìš”í•œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CSV íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
    
    # ë¡œë”© í‘œì‹œ
    with st.spinner('ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...'):
        
        # ëª¨ë“  í˜ì´ì§€ì˜ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸
        all_data = []
        
        # í˜ì´ì§€ë³„ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
        def get_page_data(page_num):
            url = 'https://kind.krx.co.kr/disclosure/todaydisclosure.do'
            params = {
                "method": "searchTodayDisclosureSub",
                "currentPageSize": 100,
                "pageIndex": page_num,
                "orderMode": 0,
                "orderStat": "D",
                "marketType": 2,  # ì½”ìŠ¤ë‹¥ì€ 2
                "forward": "todaydisclosure_sub",
                "searchMode": "",
                "searchCodeType": "",
                "chose": "S",
                "todayFlag": "Y",
                "repIsuSrtCd": "",
                "kosdaqSegment": "",
                "selDate": today_date,
                "searchCorpName": "",
                "copyUrl": ""
            }

            try:
                response = requests.post(url, params=params)
                response.raise_for_status()
                return BeautifulSoup(response.text, 'html.parser')
            except Exception as e:
                st.error(f"í˜ì´ì§€ {page_num} ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return None

        # ë°ì´í„° íŒŒì‹± í•¨ìˆ˜
        def parse_table(soup):
            data = []
            table = soup.find('table', class_='list type-00 mt10')
            
            if table and table.find('tbody'):
                for row in table.find('tbody').find_all('tr'):
                    cols = row.find_all('td')
                    
                    if len(cols) >= 5:
                        # í•„ìš”í•œ ë°ì´í„° ì¶”ì¶œ (ì›ë³¸ ì½”ë“œì™€ ë™ì¼)
                        time = cols[0].text.strip()
                        
                        company_a_tag = cols[1].find('a', id='companysum')
                        company = company_a_tag.text.strip() if company_a_tag else ""
                        
                        company_code = ""
                        if company_a_tag and company_a_tag.has_attr('onclick'):
                            onclick_attr = company_a_tag['onclick']
                            code_match = re.search(r"companysummary_open\('([A-Za-z0-9]+)'\)", onclick_attr)
                            if code_match:
                                company_code = code_match.group(1)
                        
                        title_a_tag = cols[2].find('a')
                        title = ""
                        note = ""
                        
                        if title_a_tag:
                            title = title_a_tag.get('title', "").strip()
                            
                            font_tags = title_a_tag.find_all('font')
                            if font_tags:
                                notes = []
                                for font_tag in font_tags:
                                    notes.append(font_tag.text.strip())
                                note = "_".join(notes)
                        
                        submitter = cols[3].text.strip()
                        
                        discl_url = ""
                        if title_a_tag and title_a_tag.has_attr('onclick'):
                            onclick_attr = title_a_tag['onclick']
                            match = re.search(r"openDisclsViewer\('(\d+)'", onclick_attr)
                            if match:
                                acptno = match.group(1)
                                discl_url = f"https://kind.krx.co.kr/common/disclsviewer.do?method=search&acptno={acptno}&docno=&viewerhost=&viewerport="
                        
                        data.append({
                            'ì‹œê°„': time,
                            'íšŒì‚¬ì½”ë“œ': company_code,
                            'íšŒì‚¬ëª…': company,
                            'ë¹„ê³ ': note,
                            'ê³µì‹œì œëª©': title,
                            'ì œì¶œì¸': submitter,
                            'ìƒì„¸URL': discl_url
                        })
            return data

        # ì²« í˜ì´ì§€ ìš”ì²­ ë° ë°ì´í„° ì²˜ë¦¬
        url = 'https://kind.krx.co.kr/disclosure/todaydisclosure.do'
        headers = {
            "Referer" : "https://kind.krx.co.kr/",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36"
        }
        params = {
            "method": "searchTodayDisclosureSub",
            "currentPageSize": 100,
            "pageIndex": 1,
            "orderMode": 0,
            "orderStat": "D",
            "marketType": 2,  # ì½”ìŠ¤ë‹¥ì€ 2 (ì›ë˜ ì½”ë“œì—ì„œ 1ë¡œ ì˜ëª» ì„¤ì •ë˜ì–´ ìˆì—ˆìŒ)
            "forward": "todaydisclosure_sub",
            "searchMode": "",
            "searchCodeType": "",
            "chose": "S",
            "todayFlag": "Y",
            "repIsuSrtCd": "",
            "kosdaqSegment": "",
            "selDate": today_date,
            "searchCorpName": "",
            "copyUrl": ""
        }
        
        response = requests.post(url, headers = headers, params=params)
        soup = BeautifulSoup(response.text, 'html.parser')
        st.write(soup.prettify())
        # ì´ ê±´ìˆ˜ì™€ í˜ì´ì§€ ìˆ˜ ì¶”ì¶œ
        total_items_element = soup.select_one('.info.type-00 em')
        total_pages_text = soup.select_one('.info.type-00').text.strip()
        total_pages_match = re.search(r'(\d+)/(\d+)', total_pages_text)

        if total_items_element and total_pages_match:
            total_items = int(total_items_element.text.strip().replace(",",""))
            total_pages = int(total_pages_match.group(2))
            
            st.info(f"ì¡°íšŒì¼ì— ì´ {total_items}ê±´ì˜ ê³µì‹œê°€ ìˆìŠµë‹ˆë‹¤. (ì´ {total_pages}í˜ì´ì§€)    ì§€ì›ëŒ€ìƒ ê³µì‹œëŠ” ì•„ë˜ í‘œë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.")
        else:
            st.warning("í˜ì´ì§€ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            total_pages = 1

        # ì²« í˜ì´ì§€ ë°ì´í„° ì²˜ë¦¬
        all_data.extend(parse_table(soup))

        # í˜ì´ì§€ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš° ë‚˜ë¨¸ì§€ í˜ì´ì§€ ì²˜ë¦¬
        if total_pages > 1:
            # Streamlit ì§„í–‰ í‘œì‹œì¤„
            progress_bar = st.progress(0)
            
            for i, page in enumerate(range(2, total_pages + 1)):
                page_soup = get_page_data(page)
                if page_soup:
                    page_data = parse_table(page_soup)
                    all_data.extend(page_data)
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress = (i + 1) / (total_pages - 1)
                progress_bar.progress(progress)
                
                # ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ëŒ€ê¸° ì‹œê°„
                time.sleep(0.5)

        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df_discl = pd.DataFrame(all_data)
        
        # í•„í„°ë§ (ì§€ì› ëŒ€ìƒ ì„œì‹ë§Œ í•„í„°)
        form_names = df_svc['ì„œì‹ëª…'].unique().tolist()
        
        # ì¶”ê°€ìƒì¥ì´ë‚˜ ë³€ê²½ìƒì¥ì€ ì œì™¸
        def is_contained(title):
            # ì œëª©ì´ None ë˜ëŠ” ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° ì œì™¸
            if not title:
                return False
            # 'ì¶”ê°€ìƒì¥'ì´ë‚˜ 'ë³€ê²½ìƒì¥'ìœ¼ë¡œ ì‹œì‘í•˜ë©´ ì œì™¸
            if title.startswith("ì¶”ê°€ìƒì¥") or title.startswith("ë³€ê²½ìƒì¥"):
                return False
            # ê·¸ ì™¸ì—” form_namesê°€ í¬í•¨ëœ ê²½ìš°ë§Œ True
            for form_name in form_names:
                if form_name in title:
                    return True
            return False
        # ì²« ë²ˆì§¸ í•„í„°ë§: ì§€ì› ëŒ€ìƒ ì„œì‹ë§Œ í•„í„°
        filtered_df = df_discl[df_discl['ê³µì‹œì œëª©'].apply(is_contained)]
        
        # ë‘ ë²ˆì§¸ í•„í„°ë§: ì§€ì •ëœ íšŒì‚¬ ì½”ë“œë§Œ í•„í„°
        listed_company_codes = df_listed['íšŒì‚¬ì½”ë“œ'].tolist()
        filtered_df = filtered_df[filtered_df['íšŒì‚¬ì½”ë“œ'].isin(listed_company_codes)]
        
        # ê²°ê³¼ í‘œì‹œ
        st.subheader('ì½”ìŠ¤ë‹¥ ì§€ì›ëŒ€ìƒ ê³µì‹œ ëª©ë¡')
        
        if filtered_df.empty:
            st.warning("ì¡°ê±´ì— ë§ëŠ” ê³µì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.write(f"ì´ {len(filtered_df)}ê±´ì˜ ì§€ì›ëŒ€ìƒ ê³µì‹œê°€ ìˆìŠµë‹ˆë‹¤.")
            
            # URLì„ í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ë¡œ í‘œì‹œ
            st.dataframe(
                filtered_df,
                column_config={
                    "ìƒì„¸URL": st.column_config.LinkColumn("ìƒì„¸URL"),
                },
                hide_index=True,
                use_container_width=True
            )

# ë°ì´í„° ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ (ì„ íƒì‚¬í•­)
st.sidebar.markdown("---")
if st.sidebar.button("ğŸ“Š ë°ì´í„° ìƒˆë¡œê³ ì¹¨"):
    st.cache_data.clear()
    st.rerun()

st.sidebar.info("ğŸ’¡ CSV íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
